1152    "CONV_CHANNELS"             !
16      "VISUAL_HEAD_NUM"           !           
72      "VISUAL_HEAD_LEN"           !

4900    "VISUAL_POS_NUM"            !

4304    "VISUAL_MLP_INTERNAL"       !
14      "CONV_KERNEL_SIZE"          !

%def visual_create_weights
    "CONV_CHANNELS" @ 3 14 14 4 "G_DEVICE" @ "f16" op.create "v.embeddings.patch_embedding.weight" !
    "CONV_CHANNELS" @ 1 "G_DEVICE" @ "f16" op.create "v.embeddings.patch_embedding.bias" !
    "VISUAL_POS_NUM" @ "CONV_CHANNELS" @ 2 "host" "f16" op.create "v.embeddings.position_embedding.weight" !
    
    %for 0 26
        "CONV_CHANNELS" @ 1                   "G_DEVICE" @ "f16" op.create "v.encoder.layers_%%.layer_norm1.weight" !
        "CONV_CHANNELS" @ 1                   "G_DEVICE" @ "f16" op.create "v.encoder.layers_%%.layer_norm1.bias" !
        "CONV_CHANNELS" @ 1                   "G_DEVICE" @ "f16" op.create "v.encoder.layers_%%.layer_norm2.weight" !
        "CONV_CHANNELS" @ 1                   "G_DEVICE" @ "f16" op.create "v.encoder.layers_%%.layer_norm2.bias" !
    
        "CONV_CHANNELS" @ "CONV_CHANNELS" @ 2   "G_DEVICE" @ "f16" op.create "v.encoder.layers_%%.attn.q_proj.weight" !
        "CONV_CHANNELS" @ 1                     "G_DEVICE" @ "f16" op.create "v.encoder.layers_%%.attn.q_proj.bias" !
        "CONV_CHANNELS" @ "CONV_CHANNELS" @ 2   "G_DEVICE" @ "f16" op.create "v.encoder.layers_%%.attn.k_proj.weight" !
        "CONV_CHANNELS" @ 1                     "G_DEVICE" @ "f16" op.create "v.encoder.layers_%%.attn.k_proj.bias" !

    %endf

%end

%def visual_load_one
    dup "Loading... " swap | ?

    dup @ swap "G_PATH" @ swap | ".fp16" | io.load
%end

%def visual_load_weights
    "v.embeddings.patch_embedding.weight"       visual_load_one
    "v.embeddings.patch_embedding.bias"         visual_load_one
    "v.embeddings.position_embedding.weight"    visual_load_one

    %for 0 26
        "v.encoder.layers_%%.layer_norm1.weight" visual_load_one
        "v.encoder.layers_%%.layer_norm1.bias" visual_load_one
        "v.encoder.layers_%%.layer_norm2.weight" visual_load_one
        "v.encoder.layers_%%.layer_norm2.bias" visual_load_one
        
        "v.encoder.layers_%%.attn.q_proj.weight" visual_load_one
        "v.encoder.layers_%%.attn.q_proj.bias" visual_load_one
        "v.encoder.layers_%%.attn.k_proj.weight" visual_load_one
        "v.encoder.layers_%%.attn.k_proj.bias" visual_load_one
    %endf

%end

%def visual_init
    visual_create_weights
    visual_load_weights

    ;; input image and hidden_state
    1 3 14 14336 4 "G_DEVICE" @ "f16" op.create "v_img" !
 
    ;; position and embedding
    1024 1 "host" "i32" op.create "v_ids~" !
    1 1024 "CONV_CHANNELS" @ 3 "host"       "f16" op.create "v_pos~" ! 
    1 1024 "CONV_CHANNELS" @ 3 "G_DEVICE" @ "f16" op.create "v_pos" ! 

    ;; help memory
    "CONV_CHANNELS" @ 1 "G_DEVICE" @ "f16" op.create "v.mean_c" !
    "CONV_CHANNELS" @ 1 "G_DEVICE" @ "f16" op.create "v.var_c" !

    ;; internal variable 
    1 1024 "CONV_CHANNELS" @ 3 "G_DEVICE" @ "f16" op.create "v_xa" !
    1 1024 "CONV_CHANNELS" @ 3 "G_DEVICE" @ "f16" op.create "v_xb" !
    1 1024 "CONV_CHANNELS" @ 3 "G_DEVICE" @ "f16" op.create "v_xc" !
    1 1024 "CONV_CHANNELS" @ 3 "G_DEVICE" @ "f16" op.create "v_xd" !
    
    ;; shadowed variable
    "v_xa" @ 0 1 "CONV_CHANNELS" @ 1 1024 4 op.view "v_imgx" !
    "v_xa" @ 0 1 1024 "VISUAL_HEAD_NUM" @ "VISUAL_HEAD_LEN" @ 4  op.view "v_ya" !
    "v_xb" @ 0 1 1024 "VISUAL_HEAD_NUM" @ "VISUAL_HEAD_LEN" @ 4  op.view "v_yb" !
    "v_xc" @ 0 1 1024 "VISUAL_HEAD_NUM" @ "VISUAL_HEAD_LEN" @ 4  op.view "v_yc" !
    "v_xd" @ 0 1 1024 "VISUAL_HEAD_NUM" @ "VISUAL_HEAD_LEN" @ 4  op.view "v_yd" !
    
    "v_xa" @ 0 1 "VISUAL_HEAD_NUM" @ 1024 "VISUAL_HEAD_LEN" @ 4  op.view "v_za" !
    "v_xb" @ 0 1 "VISUAL_HEAD_NUM" @ 1024 "VISUAL_HEAD_LEN" @ 4  op.view "v_zb" !
    "v_xc" @ 0 1 "VISUAL_HEAD_NUM" @ 1024 "VISUAL_HEAD_LEN" @ 4  op.view "v_zc" !
    "v_xd" @ 0 1 "VISUAL_HEAD_NUM" @ 1024 "VISUAL_HEAD_LEN" @ 4  op.view "v_zd" !
%end

%def visual_embeded
    "v_img" @ "./demo.fp16" io.load
    ;;"v_img" @ app.fill
   
    
    ;; building position
    "v_ids~" @ 70 32 32 app.position
    "v_ids~" @ "v.embeddings.position_embedding.weight" @ "v_pos~" @ op.embed
    "v_pos" @ "v_pos~" @ op.copy 

    ;; conv2d
    {
        "v_img" @ 
        "v.embeddings.patch_embedding.weight" @ 
        "v.embeddings.patch_embedding.bias" @ 
        "v_imgx" @ 14 0 op.conv2d
    }
    
    ;; change to batch | token len | hidden format
    { "v_imgx" @ 0 1 "CONV_CHANNELS" @ 1024 1 4 op.view } 
    { "v_xb" @   0 1 1024 "CONV_CHANNELS" @ 1 4 op.view } op.transpose_0213

    ;; added with position embedding 
    "v_xb" @ "v_pos" @ "v_xa" @ op.add
%end

%def visual_main
    
    ;; output is v_xa
    visual_embeded

    %for 0 26
        ;; layernorm
        "v_xa" @ "v.mean_c" @ "v.var_c" @ "v.encoder.layers_%%.layer_norm1.weight" @ "v.encoder.layers_%%.layer_norm1.bias" @ "v_xb" @ "RMS_EPS" @ op.layernorm
        
        ;; self attention
        ;; v_za is queryi && y_zc is key
        "v_xb" @ "v.encoder.layers_%%.attn.q_proj.weight" @ "v.encoder.layers_%%.attn.q_proj.bias" @ "v_xc" @ op.linear
        "v_yc" @ "v_za" @ op.transpose_0213 
       
        "v_za" @ io.dump ^

        /*
        "v_xa" @ "v.b_%%.attn.in_proj_k.weight" @ "v.b_%%.attn.in_proj_k.bias" @ "v_xd" @ op.linear
        "v_yd" @ "v_zc" @ op.transpose_0213     
  
        ;; query@key
        "v_zb" @ "v_zc" @ "v_xll" @ op.querykey
        "v_xll" @ "v_xll" @ op.softmax

        ;; v_zb is value do attn
        "v_xa" @ "v.b_%%.attn.in_proj_v.weight" @ "v.b_%%.attn.in_proj_v.bias" @ "v_xc" @ op.linear
        "v_yc" @ "v_zb" @ op.transpose_0213
        "v_xll" @ "v_zb" @ "v_zc" @ op.attn
       
        ;; post process
        "v_zc"  @ "v_yb" @ op.transpose_0213
        "v_xb"  @ "v.b_%%.attn.out_proj.weight" @ "v.b_%%.attn.out_proj.bias" @ "v_xa" @ op.linear
        "v_xinput" @ "v_xa" @ "v_xinput" @ op.add
        */
 
    
    %endf
%end

