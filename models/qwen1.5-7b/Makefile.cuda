.PHONY: all

FLAGS = -Wall -O3 -D_USING_DEVICE_CUDA_ 
INC = -I../../install/include \
	  -I${CUDA_DIR}/include \
	  -I${CUDNN_DIR}/include 

LINK = -L../../install/lib \
	   -L${CUDA_DIR}/lib64 \
	   -L${CUDNN_DIR}/lib \
	   -L/usr/lib/x86_64-linux-gnu -lreadline\
	   -ltokenizer_combo -ltokenizers_c \
	   -lssl -lcrypto \
	   -ltensortype -lcuda_kernels -lcudnn -lcudart -lcublas -lcublasLt 

all: chat

chat: chat.cpp memory.hpp ../../install/lib/libtensortype.a 
	g++ $(FLAGS) -o $@ $< $(INC) $(LINK)

run16: chat 
	LD_LIBRARY_PATH=${LD_LIBRARY_PATH}::${CUDA_DIR}/lib64:${CUDNN_DIR}/lib:${VT_SOURCE}/install/lib ./chat ./inference_fp16.dag

clean:
	rm -f chat 
